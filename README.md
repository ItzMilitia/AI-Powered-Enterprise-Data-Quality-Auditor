# **AI-Powered Enterprise Data Quality Auditor**

A multi-agent system that automatically evaluates, scores, and explains the quality of structured datasets using a **Planner â†’ Worker â†’ Evaluator** workflow.  
Designed for enterprise data engineering teams, deployed on Hugging Face Spaces, and fully open-source.

---

## ğŸš€ Live Demo

**Hugging Face Space:** *(Add your HF URL here)*  
**GitHub Repository:** https://github.com/ItzMilitia/AI-Powered-Enterprise-Data-Quality-Auditor

---

## ğŸ¯ Project Overview

Data quality is one of the biggest hidden bottlenecks in enterprise analytics and machine learning.  
Teams spend enormous time detecting and fixing issues such as:

- Missing values  
- Duplicates  
- Schema mismatches  
- Outliers  
- Incorrect data types  
- Inconsistent distributions  

This project automates that entire process through a structured **multi-agent** architecture.  
The system audits datasets, generates a **Global Data Quality Score**, and provides human-readable insights.

---

## ğŸ§  Why Agents?

Traditional scripts run checks sequentially. Data quality auditing, however, is inherently **multi-step** and **modular**, making it perfect for agent-based design:

### **Planner Agent**
Decides which checks should run based on dataset metadata.

### **Worker Agents**
Each Worker is responsible for one type of quality check:
- Missing value detection  
- Outlier detection  
- Duplicate identification  
- Schema & dtype validation  
- Basic statistical profiling  

### **Evaluator Agent**
Aggregates Worker outputs and computes:
- Severity levels  
- Recommendations  
- A final **Data Quality Score (0â€“100)**  

Agent-based design gives:
- âœ” Modularity  
- âœ” Scalability  
- âœ” Fault isolation  
- âœ” Better explainability  
- âœ” Easier future expansion  

---

## ğŸ— Architecture

User Input
â”‚
â–¼
Main Agent
â”‚
Decides: Audit Mode or Conversation Mode
â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â”‚
â–¼ â–¼
Planner Agent Conversational Response
â”‚
â–¼
Audit Plan
â”‚
â–¼
Worker Agents (parallel checks)
â”‚
â–¼
Collected Results
â”‚
â–¼
Evaluator Agent
â”‚
â–¼
Final JSON Report + Score + Summary

yaml
Copy code

---

## ğŸ§ª Features

### ğŸ” Automated Data Quality Checks
- Missing values  
- Schema / type drift  
- Duplicates  
- Outliers  
- Dataset completeness  
- Column statistics  

### ğŸ“Š Global Score
A weighted scoring model produces a final **0â€“100 quality score**.

### ğŸ“ Reporting
- JSON structured findings  
- Human-readable summaries  
- Session logs  
- Full audit history  

### ğŸ§  Memory Support
- Stores previous runs  
- Multi-turn conversations  
- Repeated queries inside same session  

### ğŸŒ Deployed App
- Gradio UI  
- Live Log panel  
- Hugging Face Space hosting  

---

## ğŸ’» Tech Stack

- Python  
- Pandas  
- Gradio  
- Loguru  
- Hugging Face Spaces  
- Custom Multi-Agent Planner â†’ Worker â†’ Evaluator pipeline  
- JSON-based A2A protocol  
- Session memory store  

---

## ğŸ” Usage Examples

### **1. Ask general questions**
What can you do?

shell
Copy code

### **2. Run a default audit**
audit my dataset

shell
Copy code

### **3. Audit a specific CSV**
Place the file at:
/content/project/test.csv

arduino
Copy code

Then run:
audit test.csv

shell
Copy code

### **4. Ask about the system**
Run a quick self-check and describe your internal workflow.

yaml
Copy code

---

## ğŸ§© Folder Structure

project/
â”‚
â”œâ”€â”€ agents/
â”‚ â”œâ”€â”€ planner.py
â”‚ â”œâ”€â”€ worker.py
â”‚ â””â”€â”€ evaluator.py
â”‚
â”œâ”€â”€ core/
â”‚ â”œâ”€â”€ context_engineering.py
â”‚ â”œâ”€â”€ a2a_protocol.py
â”‚ â””â”€â”€ observability.py
â”‚
â”œâ”€â”€ memory/
â”‚ â””â”€â”€ session_memory.py
â”‚
â”œâ”€â”€ tools/
â”‚ â””â”€â”€ tools.py
â”‚
â”œâ”€â”€ main_agent.py
â””â”€â”€ app.py

yaml
Copy code

---

## ğŸ›  How It Works

### **Step 1 â€” Planner**
Extracts metadata â†’ generates a detailed audit plan.

### **Step 2 â€” Workers**
Each Worker executes one check independently and returns structured JSON findings.

### **Step 3 â€” Evaluator**
Aggregates Worker results â†’ assigns severity â†’ calculates overall Data Quality Score.

### **Step 4 â€” Final Output**
- Summary text  
- JSON report  
- Logged process  
- Session memory updated  

---

## ğŸ“¦ Installation (Local)

```bash
git clone https://github.com/ItzMilitia/AI-Powered-Enterprise-Data-Quality-Auditor
cd AI-Powered-Enterprise-Data-Quality-Auditor
pip install -r requirements.txt
python project/run_demo.py

ğŸŒŸ If I Had More Timeâ€¦
1. LLM-Powered Insight Generator

Explain findings + propose solutions.

2. Advanced Checks

Schema drift

Data drift

Domain-based validations

Correlation & anomaly detection

3. Dashboard

Interactive visualization using Streamlit / Plotly.

4. Auto-Fix Mode

Automatically correct missing values + outliers.

5. Package Release

Publish as:

pip install dq-auditor-agent
dq-audit mydata.csv

ğŸ Final Notes

This project demonstrates:
âœ” Multi-agent design
âœ” Enterprise-grade auditing
âœ” Real-time deployment
âœ” Clean architecture
âœ” Data engineering + AI workflow automation
